\newpage
\chapter{Einleitung} 

Das Thema Datenanalyse ist fester Bestandteil in vielen Bereichen des täglichen Lebens. Die Berechnung von Stau-Wartezeiten anhand von Handy und Geo-Lokations-Daten, die Wettervorhersage oder Werbemails von Onlinehändlern, die versuchen die nächsten benötigten Artikel vorzuschlagen sind nur einige Beispiele.\\

\noindent
Durch den technologischen Fortschritt der vergangenen Jahre wurde es zu einer immer größer werdenden Herausforderung die Unmengen an Daten zu beherrschen und zu verarbeiten. Als Beispiel sind Google, Twitter, Apple und viele weitere Unternehmen mit großen Datenbeständen aufzuführen. Schon 2010 wuchs der Datenbestand bei Twitter\footnote{\textbf{Twitter} ist ein Mikrobloggingdienst. Nutzer können über das Portal Kurznachrichten verbreiten. } täglich um 12 Terabyte\footnote{Vgl. \cite{TWITTER_12}}. \\
Abhilfe schafft ein System wie Apache Spark, das in der Lage sind mit vielen Rechnern große Datenanalysen zu bewältigen. \\

\noindent
Die Fülle an wissenschaftlichen Veröffentlichungen\footnote{Vgl. \cite{SPRESEARCH}} auf der Apache Spark-Website sowie die zahlreichen Fachbücher zum Thema Datenanalyse\footnote{Vgl. \cite{DA15}} oder direkt zu Apache Spark\footnote{Vgl. \cite{AAWS15}, \cite{BDS16}} unterstreichen die Wichtigkeit und Aktualität des Themas.\\

\noindent
Ziel dieser Arbeit ist es, dass Apache Spark Framework in seinen Einzelheiten zu beleuchten und die einzelnen Komponenten vorzustellen. Des Weiteren wird auf die Performance und Fehlertoleranz näher eingegangen sowie an geeigneten Stellen kleine Beispielimplementierungen vorgestellt.
%Auf die Besonderheiten bei zum Beispiel der Performance oder Fehlertoleranz eingehen und an geeigneten Stellen kleine Beispielimplementierungen vorzustellen. \\

\noindent
Zunächst wird der Kern des Systems und der prinzipielle Aufbau des Frameworks dargelegt. Danach werden die verschiedenen Komponenten, die das Framework enthält im Detail betrachtet. Dazu zählen SQL-Abfragen für die Anbindung von Datenbanken, die Verarbeitung von Datenströmen, Berechnungen von Graphen sowie maschinelles Lernen.\\
Das Zusammenspiel mehrerer dieser Komponenten wird an einem größeren praktischen Beispiel verdeutlicht. \\

\noindent
Das Framework wird danach einer kritischen Betrachtung der Performance unterzogen, wobei Kriterien wie Speichernutzung, Netzwerkauslastung oder Datentransfer eine große Rolle spielen. Das Ende bildet ein Fazit und ein Ausblick in die Zukunft. 


%einem einzigen Rechner oder einen Großrechner kommt man jedoch nicht mehr aus. Das ist der Grund warum man versucht die Last auf viele kleine bzw. normale Rechner zu verteilen. %jedoch wird die analyse durch das verteilen nicht einfacher. Eine große sind fehlen da 
